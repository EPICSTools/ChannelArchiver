\section{Statistics} \label{sec:perfstats}
It is impossible to provide universal performance numbers for the
components of the ChannelArchiver toolset. Tests of a realistic setup
are always influenced by network delays: IOCs communicate with ArchiveEngines,
data client tools query data from the network data server.
And while the archiver tools of course share the CPU with all the
other applications that happen to run on the same CPU,
the CPU speed is less important. Most crucial is the hard
disk performance. Access to data on NFS-mounted disks is by orders of
magnitude slower than access to data on local disks.
Hard disk access is also hard to reproduce: At least under Linux, the second
run of a test is always faster because the operating system caches the
disk access. In general, the fewer files and the smaller the involved
files are, the better as far as speed is concerned, because the
operating system will cache access to files as long as memory allows.

The RTree is a balanced tree. Mathematically, this means that the
number of read requests required to locate a node in an RTree depends
on the height of the tree, which again follows logarithmically from
$M$ and the number of nodes in the tree.  An RTree with $M=50$ and
height 5 for example has one root node with 50 pointers to sub-nodes,
then up to $50^2$ nodes on the second level and so on, resulting in
access to more than $10^{80}$ records on the fifth level, that is:
with only 5 reads requests to the disk.
% 50^50 = 8.8818e+84
In practice, however, there can be a big time difference between 5
read requests to a file of 10~MB total size compared to 5 read
requests to a file of 500~MB total size, because the former could be
completely buffered by the operating system, while access to the
latter will result in individual disk access operations.

The following are performance values obtained on a computer
with a 1~GHz CPU, an ordinary IDE disk, that was mostly idle
while the archiver tools ran. % That's blitz.ta53.lanl.gov
The corresponding values on a machine with an 800~MHz CPU, concurrently
used by other people, but faster hard disks (Mylex
DAC960PTL1 PCI RAID Controller with 5 Quantum Atlas 10K drives) were
slightly better. % That's bogart.ta53.lanl.gov

We also provide some comparison to the previous architecture that used
the same data file format but instead of the RTree-based index there
were ``Directory Files''. Instead of being able to combine several sub-archives
into one index, one utilized an ASCII ``Master File'' that simply listed the
sub-archives.

\subsection{Write Performance}
As a baseline for raw data writing speed, the 'bench' program that can
be found in the ChannelArchiver/Engine directory consistently writes
at least 80000 values per second on the test computer.

\subsection{Index Performance}
\begin{figure}[htb]
\begin{center}
\InsertImage{width=\textwidth}{singleidxcompare}
\end{center}
\caption{\label{fig:singleidxcompare}RTree $M$ value tuning for small index, see text.}
\end{figure}

Performance and size of the index depend on the $M$ value configuration of
the RTree. Fig.\ \ref{fig:singleidxcompare} displays the file size and
the time needed to create an index for a small archive with 5100
channels. The samples occupy 8400 data blocks in a 12~MB data file.
The ArchiveIndexTool was used to convert the existing index file of
the archive into new indices with different $M$ values.

From the number of channels and data blocks it follows that the
samples for most channels occupy only one or two data blocks.
Consequently almost all channels can be handled by degenerated RTrees, each
with a single node that is both root and leaf of the tree, using only
1 or 2 records in that node. Any records beyond the first few 
remain unused. Fig.\ \ref{fig:singleidxcompare} clearly indicates how
the file size grows linearly with $M$ due to those unused records.
The changes in the time needed to create an index can probably be
explained as follows: After creating the first new index with
$M=3$, the time dropped observably because the operating system would from now
on cache most read requests to the original index. With growing $M$,
the time again increases caused by the growing file sizes of the new indices.
 
\begin{figure}[htb]
\begin{center}
\InsertImage{width=\textwidth}{masteridxcompare}
\end{center}
\caption{\label{fig:masteridxcompare}RTree $M$ value tuning for master
  index, see text.}
\end{figure}

Fig.\ \ref{fig:masteridxcompare} compares the file sizes and creation
time over $M$ of a master index that covers 27 sub-archives, a total
of 635~MB of data files containing 248261 data blocks for 6164
channels.  The smaller archive from the preceding section is actually
one sub-archive of this master index. Because some channels have only
very few samples, while other channels might have changed every 30
seconds, there cannot be one $M$ value that is ideal for every
channel handled by the master index. By creating the master index with
different values of $M$, we are looking for a compromise that gives
best index performance across channels.

Fig.\ \ref{fig:masteridxcompare} shows
that values between 10 and 50 result in a smaller master index than
$M$ values outside of this range. Remember that for a given height,
the number of leaf records in an RTree grows exponentially with $M$,
so slight increases of $M$ beyond 50 will vastly increase the number
of leaf records. Archives with twice or ten times the number of data
blocks will therefore not require $M$ values that are equally 2 or 10 times
bigger. Only very small increases of $M$ would be beneficial. If we
consider that in general those bigger archives will also contain
channels with only a few samples, $M=50$ will probably be ``as good''.

In the following, $M$ was kept at 50, the default for most archive tools.
\begin{itemize}
% LANL Xmtr Data 2002:
\item 12 sub-archives, 1.2~MB of old directory files, 1.4~GB Data
      files:\\
      Converting directory files into index files with $M=50$:
      Just under 3~minutes, resulting in 11~MB for the new index files.\\
      Creating a master index: 37~seconds for a master index of 9~MB.
      The master index is slightly smaller than the sum of the
      individual sub-indices because of better RTree utilization:
      The $M$ was configured to be 50 in all cases and many channels
      in the sub-archives use only a fraction of a single RTree
      node, down to an average record usage of 8\%, while the master
      index uses around 50\%.
      A re-run of the ArchiveIndexTool tool is faster because
      it detects data block that are already listed in the master index and
      therefore not added again. In this case, the re-run took 10~seconds.
% LANL Xmtr Data 2003:
\item 92 sub-archives, 12~MB directory files, 2.3GB of data files:\\
      Converting into 61~MB of index files: About 12 minutes.\\
      Creating a master index: Under 2~minutes, the resulting
      index uses about 18~MB.
      Re-run: 30~seconds.
\end{itemize}

\noindent With the 92-sub-archive index, the following was tested:
\begin{itemize}
\item Time to list all channel names: $<$1~second.\\
      This took about 7 seconds with the old ``multi archive'' file
      that required opening the individual sub-archives.
\item Time to find channels that match a pattern and show their
      start/end times: 0.2~seconds (4 channels out of 500 matched).\\
      This took about 6~seconds with the previous ``multi archive''.
\item Seek test, i.e.\ find a data block for a given start time:
      Index file requires $<$0.5~seconds, old index uses 1.5~seconds.
\end{itemize}

\subsection{Data Management Performance}
As a less-than-perfect example, we created a collection of mostly
hourly sub-archives, resulting in 297 sub-archives, 158~MB index
files, 307~MB data files.  Creation of a master index took about
25~minutes.  resulting in a master index file size of 65~MB.  A re-run
of the Index Tool took about 3~minutes.

By combining the hourly sub-archives into monthly ones, the count was
reduced from 297 sub-archives into only 16.  This took about
8~minutes, resulting in 5.5~MB for index files and 148~MB for data
files. Creation of a master index for the 16 sub-archives now took
26~seconds, a re-run was further reduced to 1.5~seconds.
Overall this shows that periodic data management, combining individual
sub-archives into fewer ones, will reduce not only the number of files
but also file sizes, resulting in better performance.

\subsection{Retrieval Performance}
Tests of the retrieval performance often include not only the
code for getting at the data but also for presenting it. In the
case of the command-line Archive Export program this would be the process
of converting time stamps and values into ASCII text and printing them.
The output was redirected to /dev/null to avoid additional penalties.

\begin{itemize}
\item Dump all the 143000 values for a channel: 4~seconds,
      translating into 35700 values per second.
\end{itemize}
